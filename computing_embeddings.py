# -*- coding: utf-8 -*-
"""Computing embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VXYIStUykQ3HUZ2aU0rj452ol1cJhQbK
"""

from collections import Counter
from sklearn.decomposition import TruncatedSVD
import numpy as np
from scipy.sparse import csr_matrix
from sklearn.datasets import fetch_20newsgroups
import pandas as pd
from scipy.spatial.distance import cosine as cosine_similarity
from matplotlib import pyplot as plt
from nltk.corpus import stopwords
import numpy as np
import pandas as pd
from scipy import sparse
from scipy.sparse import linalg 
from sklearn.preprocessing import normalize
from sklearn.metrics.pairwise import cosine_similarity
from string import punctuation
from nltk.tokenize import word_tokenize
import nltk
nltk.download('stopwords')
nltk.download('punkt')
import seaborn as sns

class UnigramFrequencies:
  def __init__(self, list_words=None):
    self.unigram_counts = Counter()
    for index, word_list in enumerate(list_words):
      for word  in word_list:
        words = word.split(" ")
        for token in words:
          self.unigram_counts[token]+=1
      
    self.word_to_index = {}
    for index, word in enumerate(self.unigram_counts.keys()):
      self.word_to_index[word] = index
      
    self.index_to_word = {}
    for index, word in enumerate(self.word_to_index.items()):
      self.index_to_word[index] = word

class SkipgramFrequencies:
    def __init__(self, word_lists, backward_window_size=2, forward_window_size=2):
        self.backward_window_size = backward_window_size
        self.forward_window_size = forward_window_size
        self.skipgram_counts = Counter()
        self.unigrams = UnigramFrequencies(word_lists)
        for index_token, word_list in enumerate(word_lists):
          word_indexes = [self.word_to_index[token] for token in word_list]
          for index, word in enumerate(word_indexes):
              context_window_start = max(0, index - self.backward_window_size)
              context_window_end = min(len(word_indexes) - 1, index + self.forward_window_size) + 1
              context = [context_idx for context_idx in range(context_window_start,context_window_end) if context_idx != index]
              for context_idx in context:
                  skipgram = (word_indexes[index], word_indexes[context_idx])
                  self.skipgram_counts[skipgram] += 1
    @property               
    def index_to_word(self):
        return self.unigrams.index_to_word
    @property
    def word_to_index(self):
        return self.unigrams.word_to_index

def calculate_pairwise_frequency_matrix(skipgrams):
    row_vals = []
    col_vals = []
    matrix_values = []
    for (index_1, index_2), skipgram_count in skipgrams.skipgram_counts.items():
        row_vals.append(index_1)
        col_vals.append(index_2)
        matrix_values.append(skipgram_count)
    sparse_m = csr_matrix((matrix_values, (row_vals, col_vals)))
    return sparse_m

def calculate_pmi_matrix(skipgrams):
    frequency_matrix = calculate_pairwise_frequency_matrix(skipgrams)
    n_skipgrams = frequency_matrix.sum()
    word_sums = np.array(frequency_matrix.sum(axis=0)).flatten()
    context_sums = np.array(frequency_matrix.sum(axis=1)).flatten()
    
    row_vals = []
    col_vals = []
    matrix_values = []

    for (skipgram_word_idx, skipgram_context_idx), skipgram_count in skipgrams.skipgram_counts.items():
        join_probability = skipgram_count / n_skipgrams
        n_word = context_sums[skipgram_word_idx]
        p_word = n_word / n_skipgrams
        n_context = word_sums[skipgram_context_idx]
        p_context = n_context / n_skipgrams 
  
        # Pointwise mututal information = log[p(w, c) / p(w)p(c)]
        pmi = np.log(join_probability / (p_word * p_context))
        row_vals.append(skipgram_word_idx)
        col_vals.append(skipgram_context_idx)
        matrix_values.append(pmi)
    return csr_matrix((matrix_values, (row_vals, col_vals)))

def calculate_word_vectors(matrix, dim=300):
    pmi_matrix = matrix
    svd = TruncatedSVD(n_components=dim, n_iter=50)
    # Use left singular vectors of PMI, scaled by eigenvalues as embeddings
    left_vectors = svd.fit_transform(pmi_matrix)
    return left_vectors * np.sqrt(svd.singular_values_)

def valid_character(word):
  stopwords_set = set(stopwords.words('english'))
  if word in stopwords_set:
    return False
  if any(p in word for p in punctuation):
        return False
  return True
  
def process_data(data):
   return [w for w in word_tokenize(data.lower()) if valid_character(w)]
  
dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers'))
tokenized_dataset = [process_data(doc) for doc in dataset.data]

skipgram_frequencies = SkipgramFrequencies(tokenized_dataset)
pmi_matrix = calculate_pmi_matrix(skipgram_frequencies)
embeddings_matrix = calculate_word_vectors(pmi_matrix, dim=300)
plt.figure(figsize=(12,5))
plt.spy(pmi_matrix[10000:15000, 10000:15000], markersize=2, color="green")
plt.title("A plot of a subset of a matrix showing the association between words(PMI Matrix)")

class MatrixNearestNeighborsIndex:
    def __init__(self, matrix, idx_to_token, token_to_idx):
        self.matrix = matrix
        self.idx_to_token = idx_to_token
        self.token_to_idx = token_to_idx
    
    def most_similar_from_label(self, query_label, n=10, return_self=False):
        query_idx = self.token_to_idx.get(query_label, None)
        if query_idx is not None:
            return self.most_similar_from_index(query_idx, n=n, return_self=return_self)

    def most_similar_from_index(self, query_idx, n=10, return_self=False):
        query_vector = self.get_vector_from_index(query_idx)
        return self.most_similar_from_vector(query_vector, n=n, query_idx=query_idx if not return_self else None)
        
    def most_similar_from_vector(self, query_vector, n=10, query_idx=None):
        if isinstance(self.matrix, csr_matrix):
            sims = cosine_similarity(self.matrix, query_vector).flatten()
        else:
            sims = self.matrix.dot(query_vector)

        sim_idxs = np.argsort(-sims)[:n + 1]
        sim_idxs = [idx for idx in sim_idxs if (query_idx is None or (query_idx is not None) and (idx != query_idx))]
        sim_word_scores = [(self.idx_to_token[sim_idx], sims[sim_idx]) for sim_idx in sim_idxs[:n]]
        return sim_word_scores

    def get_vector_from_label(self, label):
        query_idx = self.token_to_idx.get(label, None)
        if query_idx is not None:
            return self.get_vector_from_index(query_idx)
        else:
            return np.zeros(self.matrix.shape[1])

    def get_vector_from_index(self, query_idx):
        if isinstance(self.matrix, csr_matrix):
            return self.matrix.getrow(query_idx)
        else:
            return self.matrix[query_idx]

    def __getitem__(self, item):
        if isinstance(item, int):
            return self.get_vector_from_index(item)
        elif isinstance(item, str):
            return self.get_vector_from_label(item)

    def __contains__(self, item):
        return item in self.token_to_idx

nearest_neighbours = MatrixNearestNeighborsIndex(
    embeddings_matrix,
    skipgram_frequencies.index_to_word,
    skipgram_frequencies.word_to_index
)

def plot_label(xy, label, color='gray', fontsize=12):
    plt.plot(xy[0], xy[1], c=color)
    plt.text(xy[0], xy[1], label, c=color, fontsize=fontsize)


labels = ['games', 'religion', 'bank', 'math', "code"]
fig, axs = plt.subplots(2, 2, figsize=(10, 5), dpi=200)

for ii, ax in enumerate(axs.ravel()):
    label = labels[ii]
    plt.sca(ax)
    most_similar = nearest_neighbours.most_similar_from_label(label)
    for sim_label, sim_score in most_similar:
        xy = nearest_neighbours.matrix[nearest_neighbours.token_to_idx[sim_label]][:2]
        plot_label(xy, sim_label)

    xy = nearest_neighbours.matrix[nearest_neighbours.token_to_idx[label]][:2]
    plot_label(xy, label, color='green', fontsize=15)
    plt.grid()
    plt.box('on')

plt.suptitle(f'Most similar words for various queries', fontsize=12)